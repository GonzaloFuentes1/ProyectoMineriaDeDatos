{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import LinearSVC, NuSVC, SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_clean = []\n",
    "df_list = []\n",
    "for i in range(15):\n",
    "    df = pd.read_csv(f\"Datasets/Duplicated{i+1}MeanPastMatches.csv\")\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    df.dropna(inplace=True)\n",
    "    index_limpio = set(df[\"index\"])\n",
    "\n",
    "    df_list.append(df)\n",
    "    index_clean.append(index_limpio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_set = index_clean[0]\n",
    "for i in index_clean:\n",
    "    init_set = init_set.intersection(i)\n",
    "final_set = list(init_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62093"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En este caso X_train, X_val y X_test son listas de dataframes\n",
    "X_train, X_val, X_test = [], [], []\n",
    "y_train, y_val, y_test = [], [], []\n",
    "\n",
    "for i in range(15):\n",
    "    df = df_list[i][df_list[i][\"index\"].isin(final_set)]\n",
    "    df = pd.get_dummies(df, columns=[\"surface\", \"player1_hand\", \"player2_hand\", \"best_of\"], dtype=int)\n",
    "    X = df.drop([\"label\"],axis = 1)\n",
    "    y = df[\"label\"]\n",
    "\n",
    "    X_train_df = X[X[\"tourney_date\"] < 20150101].drop(\"tourney_date\", axis=1) # 2000 - 2014\n",
    "    X_val_df = X[(X[\"tourney_date\"]>= 20150101) & (X[\"tourney_date\"] < 20190101)].drop(\"tourney_date\", axis=1) # 2015 - 2018\n",
    "    X_test_df = X[X[\"tourney_date\"] >= 20190101].drop(\"tourney_date\", axis=1) # 2018 - 2024\n",
    "\n",
    "    y_train_aux, y_val_aux, y_test_aux = y[X_train_df.index], y[X_val_df.index], y[X_test_df.index]\n",
    "    \n",
    "    X_train.append(X_train_df)\n",
    "    X_val.append(X_val_df)\n",
    "    X_test.append(X_test_df)\n",
    "\n",
    "    y_train.append(y_train_aux)\n",
    "    y_val.append(y_val_aux)\n",
    "    y_test.append(y_test_aux)\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño de los sets: \n",
      " Set training 0.6465140998180149 \n",
      " Set val 0.17191954004477156 \n",
      " Set test 0.18156636013721353\n"
     ]
    }
   ],
   "source": [
    "total = len(X_train[1]) + len(X_val[1]) + len(X_test[1])\n",
    "print(f\"Tamaño de los sets: \\n Set training {len(X_train[1])/total} \\n Set val {len(X_val[1])/total} \\n Set test {len(X_test[1])/total}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.5\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "# Veamos el desbalance en cada particion, deberia ser totalmente balanceada\n",
    "\n",
    "print(sum(y_train[0]==1) / len(y_train[0]))\n",
    "\n",
    "print(sum(y_val[0]==1) / len(y_val[0]))\n",
    "\n",
    "print(sum(y_test[0]==1) / len(y_test[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Set número 1\n",
      "Nuevo mejor modelo (2, 3), con 0.6453864168618267\n",
      "Nuevo mejor modelo (2, 6), con 0.6466510538641687\n",
      "Nuevo mejor modelo (2, 9), con 0.6492740046838408\n",
      "Nuevo mejor modelo (2, 12), con 0.6518032786885246\n",
      "Nuevo mejor modelo (2, 15), con 0.6550351288056206\n",
      "\n",
      "Set número 2\n",
      "Nuevo mejor modelo (2, 3), con 0.6439812646370023\n",
      "Nuevo mejor modelo (2, 6), con 0.6457611241217799\n",
      "Nuevo mejor modelo (2, 9), con 0.6506791569086651\n",
      "Nuevo mejor modelo (2, 12), con 0.6513348946135832\n",
      "Nuevo mejor modelo (2, 15), con 0.6523185011709602\n",
      "Nuevo mejor modelo (2, 21), con 0.6527400468384075\n",
      "Nuevo mejor modelo (2, 24), con 0.6532084309133489\n",
      "Nuevo mejor modelo (2, 27), con 0.6534426229508197\n",
      "\n",
      "Set número 3\n",
      "Nuevo mejor modelo (2, 3), con 0.6441686182669789\n",
      "Nuevo mejor modelo (2, 6), con 0.645480093676815\n",
      "Nuevo mejor modelo (2, 9), con 0.6533021077283372\n",
      "Nuevo mejor modelo (2, 12), con 0.6540046838407494\n",
      "Nuevo mejor modelo (2, 15), con 0.6543325526932084\n",
      "Nuevo mejor modelo (2, 18), con 0.6553161592505855\n",
      "\n",
      "Set número 4\n",
      "Nuevo mejor modelo (2, 3), con 0.6449180327868852\n",
      "Nuevo mejor modelo (2, 6), con 0.6494613583138173\n",
      "Nuevo mejor modelo (2, 9), con 0.6521779859484778\n",
      "Nuevo mejor modelo (2, 15), con 0.6556908665105387\n",
      "Nuevo mejor modelo (2, 21), con 0.6557377049180327\n",
      "Nuevo mejor modelo (2, 24), con 0.6562060889929743\n",
      "\n",
      "Set número 5\n",
      "Nuevo mejor modelo (2, 3), con 0.6457142857142857\n",
      "Nuevo mejor modelo (2, 6), con 0.6490866510538642\n",
      "Nuevo mejor modelo (2, 9), con 0.6493208430913349\n",
      "Nuevo mejor modelo (2, 12), con 0.6540515222482436\n",
      "Nuevo mejor modelo (2, 15), con 0.6542857142857142\n",
      "\n",
      "Set número 6\n",
      "Nuevo mejor modelo (2, 3), con 0.6426697892271662\n",
      "Nuevo mejor modelo (2, 6), con 0.6469320843091335\n",
      "Nuevo mejor modelo (2, 9), con 0.6503512880562061\n",
      "Nuevo mejor modelo (2, 12), con 0.6517564402810304\n",
      "Nuevo mejor modelo (2, 15), con 0.6533021077283372\n",
      "Nuevo mejor modelo (2, 18), con 0.6546135831381733\n",
      "Nuevo mejor modelo (3, 12), con 0.6550351288056206\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[106], line 37\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t_ \u001b[38;5;129;01min\u001b[39;00m range_T:\n\u001b[0;32m     36\u001b[0m     clf \u001b[38;5;241m=\u001b[39m AdaBoostClassifier(DecisionTreeClassifier(max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m), n_estimators\u001b[38;5;241m=\u001b[39mt_, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m     \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mset_X_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mset_y_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict(set_X_val)\n\u001b[0;32m     39\u001b[0m     acc \u001b[38;5;241m=\u001b[39m accuracy_score(set_y_val, y_pred)\n",
      "File \u001b[1;32mc:\\Users\\gfuen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\gfuen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:171\u001b[0m, in \u001b[0;36mBaseWeightBoosting.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    168\u001b[0m sample_weight[zero_weight_mask] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;66;03m# Boosting step\u001b[39;00m\n\u001b[1;32m--> 171\u001b[0m sample_weight, estimator_weight, estimator_error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_boost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43miboost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;66;03m# Early termination\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\gfuen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:579\u001b[0m, in \u001b[0;36mAdaBoostClassifier._boost\u001b[1;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Implement a single boost.\u001b[39;00m\n\u001b[0;32m    541\u001b[0m \n\u001b[0;32m    542\u001b[0m \u001b[38;5;124;03mPerform a single boost according to the real multi-class SAMME.R\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    576\u001b[0m \u001b[38;5;124;03m    If None then boosting has terminated early.\u001b[39;00m\n\u001b[0;32m    577\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    578\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malgorithm \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSAMME.R\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 579\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_boost_real\u001b[49m\u001b[43m(\u001b[49m\u001b[43miboost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    581\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# elif self.algorithm == \"SAMME\":\u001b[39;00m\n\u001b[0;32m    582\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_boost_discrete(iboost, X, y, sample_weight, random_state)\n",
      "File \u001b[1;32mc:\\Users\\gfuen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:588\u001b[0m, in \u001b[0;36mAdaBoostClassifier._boost_real\u001b[1;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Implement a single boost using the SAMME.R real algorithm.\"\"\"\u001b[39;00m\n\u001b[0;32m    586\u001b[0m estimator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[1;32m--> 588\u001b[0m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    590\u001b[0m y_predict_proba \u001b[38;5;241m=\u001b[39m estimator\u001b[38;5;241m.\u001b[39mpredict_proba(X)\n\u001b[0;32m    592\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m iboost \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\gfuen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\gfuen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\tree\\_classes.py:959\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    928\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    929\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    930\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    931\u001b[0m \n\u001b[0;32m    932\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    956\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m    957\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 959\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    961\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    962\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    963\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    965\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\gfuen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\tree\\_classes.py:443\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    433\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    434\u001b[0m         splitter,\n\u001b[0;32m    435\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    440\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    441\u001b[0m     )\n\u001b[1;32m--> 443\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "range_T = [3*(i+1) for i in range(10)] # Modelos base\n",
    "hiperparameters = {}\n",
    "for i in range(len(X_train)):\n",
    "    print(f\"\\nSet número {i+1}\")\n",
    "    # Por cada info\n",
    "    set_X_train = X_train[i]\n",
    "    set_X_val = X_val[i]\n",
    "\n",
    "    set_y_train = y_train[i]\n",
    "    set_y_val = y_val[i]\n",
    "\n",
    "    best_tuple = ()\n",
    "    best_acc = 0\n",
    "\n",
    "\n",
    "    for t_ in range_T:\n",
    "        clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2), n_estimators=t_, random_state=0)\n",
    "        clf.fit(set_X_train, set_y_train)\n",
    "        y_pred = clf.predict(set_X_val)\n",
    "        acc = accuracy_score(set_y_val, y_pred)\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_tuple = (2 , t_)\n",
    "            print(f\"Nuevo mejor modelo {best_tuple}, con {best_acc}\")\n",
    "        \n",
    "    for t_ in range_T:\n",
    "        clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=3), n_estimators=t_, random_state=0)\n",
    "        clf.fit(set_X_train, set_y_train)\n",
    "        y_pred = clf.predict(set_X_val)\n",
    "        acc = accuracy_score(set_y_val, y_pred)\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_tuple = (3 , t_)\n",
    "            print(f\"Nuevo mejor modelo {best_tuple}, con {best_acc}\")\n",
    "    for t_ in range_T:\n",
    "        clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=4), n_estimators=t_, random_state=0)\n",
    "        clf.fit(set_X_train, set_y_train)\n",
    "        y_pred = clf.predict(set_X_val)\n",
    "        acc = accuracy_score(set_y_val, y_pred)\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_tuple = (4 , t_)\n",
    "            print(f\"Nuevo mejor modelo {best_tuple}, con {best_acc}\")\n",
    "\n",
    "    #Guardamos los hiperparámetros\n",
    "    hiperparameters[i+1] = best_tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ahora que tenemos los mejores hiperparámetros, hacemos un modelo a partir de ese\n",
    "class MetaEnsamble:\n",
    "\n",
    "    def __init__(self, hiperparameters):\n",
    "        self.hiperparameters = hiperparameters\n",
    "\n",
    "    def fitAdaBoost(self, X_train, y_train):\n",
    "        self.models = []\n",
    "        for i in range(len(self.hiperparameters)):\n",
    "            depth, t = self.hiperparameters[i+1]\n",
    "            model = AdaBoostClassifier(DecisionTreeClassifier(max_depth=depth), n_estimators=t, random_state=0)\n",
    "            model.fit(X_train[i], y_train[i])\n",
    "            self.models.append(model)\n",
    "\n",
    "    def fit(self, X_train, y_train, mtype = \"logistic\", nu = 0.1, depth = 2, n_estimators = 10): # mtype es el tipo de MetaEnsamble a probar\n",
    "        X = []\n",
    "        for i in range(len(self.models)):\n",
    "            prediction = self.models[i].predict(X_train[i])\n",
    "            X.append(pd.DataFrame(prediction))\n",
    "        X = pd.concat(X, axis=1)\n",
    "\n",
    "        if mtype == \"logistic\":\n",
    "            self.model = LogisticRegression()\n",
    "            self.model.fit(X, y_train[0])\n",
    "\n",
    "        if mtype == \"mlp\":\n",
    "            self.model = MLPClassifier(hidden_layer_sizes=(64, 64, 64), activation= \"relu\", max_iter=1000, random_state=0)\n",
    "            self.model.fit(X, y_train[0])\n",
    "        \n",
    "        if mtype == \"svm-linear\":\n",
    "            self.model = NuSVC(nu = nu)\n",
    "            self.model.fit(X, y_train[0])\n",
    "\n",
    "        if mtype == \"adaboost\":\n",
    "            self.model = AdaBoostClassifier(DecisionTreeClassifier(max_depth=depth), n_estimators=n_estimators, random_state=0)\n",
    "            self.model.fit(X, y_train[0])\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        X = []\n",
    "        for i in range(len(self.models)):\n",
    "            prediction = self.models[i].predict(X_test[i])\n",
    "            X.append(pd.DataFrame(prediction))\n",
    "        X = pd.concat(X, axis=1)\n",
    "        return self.model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hiperparameters = {1:(2, 15), 2:(2, 35), 3:(2, 20),\n",
    "#                    4:(2, 25), 5:(2, 15), 6:(2, 20),\n",
    "#                    7:(2, 20), 8:(2, 25), 9:(2, 35),\n",
    "#                    10:(2, 15)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6585948477751756"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ahora, hacemos el meta-ensamble:\n",
    "modelLogistic = MetaEnsamble(hiperparameters)\n",
    "modelLogistic.fitAdaBoost(X_train, y_train)\n",
    "modelLogistic.fit(X_train, y_train, mtype=\"logistic\")\n",
    "\n",
    "y_pred = modelLogistic.predict(X_val)\n",
    "accuracy_score(y_val[0], y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6544730679156908"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ahora, hacemos el meta-ensamble:\n",
    "modelMLP = MetaEnsamble(hiperparameters)\n",
    "modelMLP.fitAdaBoost(X_train, y_train)\n",
    "modelMLP.fit(X_train, y_train, mtype=\"mlp\")\n",
    "\n",
    "y_pred = modelMLP.predict(X_val)\n",
    "accuracy_score(y_val[0], y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6431850117096019"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ahora, hacemos el meta-ensamble:\n",
    "modelSVM = MetaEnsamble(hiperparameters)\n",
    "modelSVM.fitAdaBoost(X_train, y_train)\n",
    "modelSVM.fit(X_train, y_train, mtype=\"svm-linear\", nu = 0.1)\n",
    "\n",
    "y_pred = modelSVM.predict(X_val)\n",
    "accuracy_score(y_val[0], y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_T = [3*(i+1) for i in range(10)] # Modelos base\n",
    "\n",
    "for t_ in range_T:\n",
    "    modelSVM = MetaEnsamble(hiperparameters)\n",
    "    modelSVM.fitAdaBoost(X_train, y_train)\n",
    "    modelSVM.fit(X_train, y_train, mtype=\"adaboost\", depth = 2, n_estimators= t_)\n",
    "\n",
    "    y_pred = modelSVM.predict(X_val)\n",
    "    print(f\"Depth {2}, estimators {t_}\")\n",
    "    print(accuracy_score(y_val[0], y_pred))\n",
    "\n",
    "    modelSVM = MetaEnsamble(hiperparameters)\n",
    "    modelSVM.fitAdaBoost(X_train, y_train)\n",
    "    modelSVM.fit(X_train, y_train, mtype=\"adaboost\", depth = 3, n_estimators= t_)\n",
    "\n",
    "    y_pred = modelSVM.predict(X_val)\n",
    "    print(f\"Depth {3}, estimators {t_}\")\n",
    "    print(accuracy_score(y_val[0], y_pred))\n",
    "\n",
    "    modelSVM = MetaEnsamble(hiperparameters)\n",
    "    modelSVM.fitAdaBoost(X_train, y_train)\n",
    "    modelSVM.fit(X_train, y_train, mtype=\"adaboost\", depth = 4, n_estimators= t_)\n",
    "\n",
    "    print(f\"Depth {4}, estimators {t_}\")\n",
    "    y_pred = modelSVM.predict(X_val)\n",
    "    print(accuracy_score(y_val[0], y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
