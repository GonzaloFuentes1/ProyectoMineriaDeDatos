{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import LinearSVC, NuSVC, SVC\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_clean = []\n",
    "df_list = []\n",
    "for i in range(15):\n",
    "    df = pd.read_csv(f\"Datasets/Duplicated{i+1}MeanPastMatches.csv\")\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    df.dropna(inplace=True)\n",
    "    index_limpio = set(df[\"index\"])\n",
    "\n",
    "    df_list.append(df)\n",
    "    index_clean.append(index_limpio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_set = index_clean[0]\n",
    "for i in index_clean:\n",
    "    init_set = init_set.intersection(i)\n",
    "final_set = list(init_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62117"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En este caso X_train, X_val y X_test son listas de dataframes\n",
    "X_train, X_val, X_test = [], [], []\n",
    "y_train, y_val, y_test = [], [], []\n",
    "\n",
    "for i in range(15):\n",
    "    df = df_list[i][df_list[i][\"index\"].isin(final_set)]\n",
    "    df = pd.get_dummies(df, columns=[\"surface\", \"player1_hand\", \"player2_hand\", \"best_of\"], dtype=int)\n",
    "    X = df.drop([\"label\"],axis = 1)\n",
    "    y = df[\"label\"]\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    X_train_df = X[X[\"tourney_date\"] < 20150101].drop(\"tourney_date\", axis=1) # 2000 - 2014\n",
    "    X_val_df = X[(X[\"tourney_date\"]>= 20150101) & (X[\"tourney_date\"] < 20190101)].drop(\"tourney_date\", axis=1) # 2015 - 2018\n",
    "    X_test_df = X[X[\"tourney_date\"] >= 20190101].drop(\"tourney_date\", axis=1) # 2018 - 2024\n",
    "\n",
    "    y_train_aux, y_val_aux, y_test_aux = y[X_train_df.index], y[X_val_df.index], y[X_test_df.index]\n",
    "    \n",
    "    # Normalizacion\n",
    "    X_train_df = scaler.fit_transform(X_train_df)\n",
    "    X_val_df = scaler.transform(X_val_df)\n",
    "    X_test_df = scaler.transform(X_test_df)\n",
    "    \n",
    "    X_train.append(X_train_df)\n",
    "    X_val.append(X_val_df)\n",
    "    X_test.append(X_test_df)\n",
    "\n",
    "    y_train.append(y_train_aux)\n",
    "    y_val.append(y_val_aux)\n",
    "    y_test.append(y_test_aux)\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño de los sets: \n",
      " Set training 0.6466506753384742 \n",
      " Set val 0.17185311589419966 \n",
      " Set test 0.18149620876732617\n"
     ]
    }
   ],
   "source": [
    "total = len(X_train[1]) + len(X_val[1]) + len(X_test[1])\n",
    "print(f\"Tamaño de los sets: \\n Set training {len(X_train[1])/total} \\n Set val {len(X_val[1])/total} \\n Set test {len(X_test[1])/total}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.5\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "# Veamos el desbalance en cada particion, deberia ser totalmente balanceada\n",
    "\n",
    "print(sum(y_train[0]==1) / len(y_train[0]))\n",
    "\n",
    "print(sum(y_val[0]==1) / len(y_val[0]))\n",
    "\n",
    "print(sum(y_test[0]==1) / len(y_test[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Set número 1\n",
      "Nuevo mejor modelo (2, 3), con 0.6453864168618267\n",
      "Nuevo mejor modelo (2, 6), con 0.647447306791569\n",
      "Nuevo mejor modelo (2, 9), con 0.6526932084309134\n",
      "Nuevo mejor modelo (2, 12), con 0.6539110070257611\n",
      "Nuevo mejor modelo (2, 15), con 0.6552224824355972\n",
      "Nuevo mejor modelo (2, 18), con 0.6586885245901639\n",
      "Nuevo mejor modelo (2, 24), con 0.6592505854800936\n",
      "Nuevo mejor modelo (2, 27), con 0.6622014051522248\n",
      "\n",
      "Set número 2\n",
      "Nuevo mejor modelo (2, 3), con 0.6439812646370023\n",
      "Nuevo mejor modelo (2, 6), con 0.6477751756440281\n",
      "Nuevo mejor modelo (2, 9), con 0.6521779859484778\n",
      "Nuevo mejor modelo (2, 12), con 0.6540046838407494\n",
      "Nuevo mejor modelo (2, 15), con 0.6576580796252928\n",
      "Nuevo mejor modelo (3, 21), con 0.6579391100702576\n",
      "Nuevo mejor modelo (3, 24), con 0.6579859484777517\n",
      "\n",
      "Set número 3\n",
      "Nuevo mejor modelo (2, 3), con 0.6441686182669789\n",
      "Nuevo mejor modelo (2, 6), con 0.6465105386416862\n",
      "Nuevo mejor modelo (2, 9), con 0.6548946135831382\n",
      "Nuevo mejor modelo (2, 12), con 0.6556440281030445\n",
      "Nuevo mejor modelo (2, 15), con 0.6564402810304449\n",
      "Nuevo mejor modelo (2, 21), con 0.65807962529274\n",
      "\n",
      "Set número 4\n",
      "Nuevo mejor modelo (2, 3), con 0.6449180327868852\n",
      "Nuevo mejor modelo (2, 6), con 0.6492271662763466\n",
      "Nuevo mejor modelo (2, 9), con 0.6531147540983606\n",
      "Nuevo mejor modelo (2, 12), con 0.654192037470726\n",
      "Nuevo mejor modelo (2, 15), con 0.6550819672131147\n",
      "Nuevo mejor modelo (2, 18), con 0.6565807962529274\n",
      "Nuevo mejor modelo (2, 21), con 0.6570023419203747\n",
      "Nuevo mejor modelo (2, 24), con 0.6592037470725995\n",
      "Nuevo mejor modelo (2, 27), con 0.6597189695550352\n",
      "Nuevo mejor modelo (2, 45), con 0.6601873536299766\n",
      "Nuevo mejor modelo (3, 30), con 0.6604683840749415\n",
      "\n",
      "Set número 5\n",
      "Nuevo mejor modelo (2, 3), con 0.6457142857142857\n",
      "Nuevo mejor modelo (2, 6), con 0.6498829039812647\n",
      "Nuevo mejor modelo (2, 9), con 0.6520374707259953\n",
      "Nuevo mejor modelo (2, 12), con 0.6541451990632319\n",
      "Nuevo mejor modelo (2, 18), con 0.6582669789227166\n",
      "Nuevo mejor modelo (2, 42), con 0.6590632318501171\n",
      "\n",
      "Set número 6\n",
      "Nuevo mejor modelo (2, 3), con 0.6426229508196721\n",
      "Nuevo mejor modelo (2, 6), con 0.6471662763466042\n",
      "Nuevo mejor modelo (2, 9), con 0.6517564402810304\n",
      "Nuevo mejor modelo (2, 12), con 0.6525058548009368\n",
      "Nuevo mejor modelo (2, 15), con 0.654192037470726\n",
      "Nuevo mejor modelo (2, 18), con 0.656768149882904\n",
      "Nuevo mejor modelo (2, 21), con 0.6575644028103045\n",
      "Nuevo mejor modelo (2, 27), con 0.657751756440281\n",
      "Nuevo mejor modelo (2, 30), con 0.6593911007025761\n",
      "Nuevo mejor modelo (2, 33), con 0.6602810304449649\n",
      "Nuevo mejor modelo (2, 36), con 0.6611709601873537\n",
      "\n",
      "Set número 7\n",
      "Nuevo mejor modelo (2, 3), con 0.6408430913348946\n",
      "Nuevo mejor modelo (2, 6), con 0.6481498829039812\n",
      "Nuevo mejor modelo (2, 9), con 0.6517564402810304\n",
      "Nuevo mejor modelo (2, 12), con 0.6548946135831382\n",
      "Nuevo mejor modelo (2, 15), con 0.6579859484777517\n",
      "Nuevo mejor modelo (2, 18), con 0.6584074941451991\n",
      "Nuevo mejor modelo (2, 21), con 0.6594379391100702\n",
      "Nuevo mejor modelo (2, 24), con 0.6597189695550352\n",
      "Nuevo mejor modelo (2, 27), con 0.6604215456674473\n",
      "Nuevo mejor modelo (2, 30), con 0.6628571428571428\n",
      "Nuevo mejor modelo (2, 33), con 0.6634660421545667\n",
      "\n",
      "Set número 8\n",
      "Nuevo mejor modelo (2, 3), con 0.6377049180327868\n",
      "Nuevo mejor modelo (2, 6), con 0.6463231850117096\n",
      "Nuevo mejor modelo (2, 9), con 0.6493676814988291\n",
      "Nuevo mejor modelo (2, 12), con 0.6503044496487119\n",
      "Nuevo mejor modelo (2, 15), con 0.6527868852459017\n",
      "Nuevo mejor modelo (2, 18), con 0.6550351288056206\n",
      "Nuevo mejor modelo (2, 24), con 0.6552693208430913\n",
      "Nuevo mejor modelo (2, 27), con 0.6570491803278689\n",
      "Nuevo mejor modelo (2, 42), con 0.6573302107728337\n",
      "\n",
      "Set número 9\n",
      "Nuevo mejor modelo (2, 3), con 0.6400468384074941\n",
      "Nuevo mejor modelo (2, 6), con 0.6475409836065574\n",
      "Nuevo mejor modelo (2, 9), con 0.6507728337236534\n",
      "Nuevo mejor modelo (2, 12), con 0.6544262295081967\n",
      "Nuevo mejor modelo (2, 15), con 0.6547540983606558\n",
      "Nuevo mejor modelo (2, 18), con 0.6565339578454332\n",
      "Nuevo mejor modelo (2, 27), con 0.6573302107728337\n",
      "Nuevo mejor modelo (2, 45), con 0.6576112412177986\n",
      "\n",
      "Set número 10\n",
      "Nuevo mejor modelo (2, 3), con 0.6368618266978923\n",
      "Nuevo mejor modelo (2, 6), con 0.6492740046838408\n",
      "Nuevo mejor modelo (2, 9), con 0.65288056206089\n",
      "Nuevo mejor modelo (2, 15), con 0.6548946135831382\n",
      "Nuevo mejor modelo (2, 18), con 0.6565339578454332\n",
      "Nuevo mejor modelo (2, 21), con 0.6583138173302108\n",
      "Nuevo mejor modelo (3, 15), con 0.6602341920374707\n",
      "\n",
      "Set número 11\n",
      "Nuevo mejor modelo (2, 3), con 0.6426697892271662\n",
      "Nuevo mejor modelo (2, 6), con 0.6456674473067916\n",
      "Nuevo mejor modelo (2, 9), con 0.6527868852459017\n",
      "Nuevo mejor modelo (2, 12), con 0.6529742388758782\n",
      "Nuevo mejor modelo (2, 15), con 0.6544730679156908\n",
      "Nuevo mejor modelo (2, 18), con 0.6555971896955504\n",
      "Nuevo mejor modelo (2, 21), con 0.6565807962529274\n",
      "Nuevo mejor modelo (2, 24), con 0.6584543325526933\n",
      "\n",
      "Set número 12\n",
      "Nuevo mejor modelo (2, 3), con 0.6424355971896956\n",
      "Nuevo mejor modelo (2, 6), con 0.6455737704918033\n",
      "Nuevo mejor modelo (2, 9), con 0.6526463700234192\n",
      "Nuevo mejor modelo (2, 12), con 0.6526932084309134\n",
      "Nuevo mejor modelo (2, 15), con 0.6533957845433255\n",
      "Nuevo mejor modelo (2, 18), con 0.6556908665105387\n",
      "Nuevo mejor modelo (2, 21), con 0.6562060889929743\n",
      "Nuevo mejor modelo (2, 36), con 0.656768149882904\n",
      "Nuevo mejor modelo (3, 15), con 0.6585011709601873\n",
      "\n",
      "Set número 13\n",
      "Nuevo mejor modelo (2, 3), con 0.6416861826697893\n",
      "Nuevo mejor modelo (2, 6), con 0.6465573770491804\n",
      "Nuevo mejor modelo (2, 9), con 0.6504918032786885\n",
      "Nuevo mejor modelo (2, 12), con 0.6536299765807962\n",
      "Nuevo mejor modelo (2, 15), con 0.6541451990632319\n",
      "Nuevo mejor modelo (2, 21), con 0.6566744730679157\n",
      "Nuevo mejor modelo (2, 24), con 0.6568618266978923\n",
      "Nuevo mejor modelo (2, 27), con 0.6572833723653396\n",
      "Nuevo mejor modelo (2, 30), con 0.6582201405152225\n",
      "Nuevo mejor modelo (2, 36), con 0.6595784543325527\n",
      "Nuevo mejor modelo (2, 39), con 0.6605152224824355\n",
      "Nuevo mejor modelo (2, 42), con 0.661311475409836\n",
      "\n",
      "Set número 14\n",
      "Nuevo mejor modelo (2, 3), con 0.6447306791569086\n",
      "Nuevo mejor modelo (2, 6), con 0.6496487119437939\n",
      "Nuevo mejor modelo (2, 9), con 0.652271662763466\n",
      "Nuevo mejor modelo (2, 12), con 0.6540983606557377\n",
      "Nuevo mejor modelo (2, 15), con 0.6552224824355972\n",
      "Nuevo mejor modelo (2, 21), con 0.6556908665105387\n",
      "Nuevo mejor modelo (2, 24), con 0.6565807962529274\n",
      "Nuevo mejor modelo (2, 27), con 0.657423887587822\n",
      "\n",
      "Set número 15\n",
      "Nuevo mejor modelo (2, 3), con 0.6439344262295082\n",
      "Nuevo mejor modelo (2, 6), con 0.6474941451990632\n",
      "Nuevo mejor modelo (2, 9), con 0.6520843091334895\n",
      "Nuevo mejor modelo (2, 12), con 0.6537704918032787\n",
      "Nuevo mejor modelo (2, 18), con 0.6565339578454332\n",
      "Nuevo mejor modelo (2, 21), con 0.6565807962529274\n",
      "Nuevo mejor modelo (2, 27), con 0.6573770491803279\n",
      "Nuevo mejor modelo (2, 33), con 0.6586416861826698\n",
      "Nuevo mejor modelo (2, 42), con 0.6594379391100702\n",
      "Nuevo mejor modelo (2, 45), con 0.6602341920374707\n"
     ]
    }
   ],
   "source": [
    "range_T = [3*(i+1) for i in range(10)] # Modelos base\n",
    "hiperparameters = {}\n",
    "\n",
    "for i in range(len(X_train)):\n",
    "    print(f\"\\nSet número {i+1}\")\n",
    "    # Por cada info\n",
    "    set_X_train = X_train[i]\n",
    "    set_X_val = X_val[i]\n",
    "\n",
    "    set_y_train = y_train[i]\n",
    "    set_y_val = y_val[i]\n",
    "\n",
    "    best_tuple = ()\n",
    "    best_acc = 0\n",
    "\n",
    "\n",
    "    for t_ in range_T:\n",
    "        clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2), n_estimators=t_, random_state=0)\n",
    "        clf.fit(set_X_train, set_y_train)\n",
    "        y_pred = clf.predict(set_X_val)\n",
    "        acc = accuracy_score(set_y_val, y_pred)\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_tuple = (2 , t_)\n",
    "            print(f\"Nuevo mejor modelo {best_tuple}, con {best_acc}\")\n",
    "        \n",
    "    for t_ in range_T:\n",
    "        clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=3), n_estimators=t_, random_state=0)\n",
    "        clf.fit(set_X_train, set_y_train)\n",
    "        y_pred = clf.predict(set_X_val)\n",
    "        acc = accuracy_score(set_y_val, y_pred)\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_tuple = (3 , t_)\n",
    "            print(f\"Nuevo mejor modelo {best_tuple}, con {best_acc}\")\n",
    "    for t_ in range_T:\n",
    "        clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=4), n_estimators=t_, random_state=0)\n",
    "        clf.fit(set_X_train, set_y_train)\n",
    "        y_pred = clf.predict(set_X_val)\n",
    "        acc = accuracy_score(set_y_val, y_pred)\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_tuple = (4 , t_)\n",
    "            print(f\"Nuevo mejor modelo {best_tuple}, con {best_acc}\")\n",
    "\n",
    "    #Guardamos los hiperparámetros\n",
    "    hiperparameters[i+1] = best_tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ahora que tenemos los mejores hiperparámetros, hacemos un modelo a partir de ese\n",
    "class MetaEnsamble:\n",
    "\n",
    "    def __init__(self, hiperparameters):\n",
    "        self.hiperparameters = hiperparameters\n",
    "\n",
    "    def fitAdaBoost(self, X_train, y_train):\n",
    "        self.models = []\n",
    "        for i in range(len(self.hiperparameters)):\n",
    "            depth, t = self.hiperparameters[i+1]\n",
    "            model = AdaBoostClassifier(DecisionTreeClassifier(max_depth=depth), n_estimators=t, random_state=0)\n",
    "            model.fit(X_train[i], y_train[i])\n",
    "            self.models.append(model)\n",
    "\n",
    "    def fit(self, X_train, y_train, mtype = \"logistic\", nu = 0.1, depth = 2, n_estimators = 10): # mtype es el tipo de MetaEnsamble a probar\n",
    "        X = []\n",
    "        for i in range(len(self.models)):\n",
    "            prediction = self.models[i].predict(X_train[i])\n",
    "            X.append(pd.DataFrame(prediction))\n",
    "        X = pd.concat(X, axis=1)\n",
    "\n",
    "        if mtype == \"logistic\":\n",
    "            self.model = LogisticRegression()\n",
    "            self.model.fit(X, y_train[0])\n",
    "\n",
    "        if mtype == \"mlp\":\n",
    "            self.model = MLPClassifier(hidden_layer_sizes=(16, 16, 16), activation= \"relu\", max_iter=2000, random_state=0)\n",
    "            self.model.fit(X, y_train[0])\n",
    "        \n",
    "        if mtype == \"svm-linear\":\n",
    "            self.model = NuSVC(nu = nu)\n",
    "            self.model.fit(X, y_train[0])\n",
    "\n",
    "        if mtype == \"svm-rbf\":\n",
    "            self.model = NuSVC(nu = nu, kernel=\"rbf\")\n",
    "            self.model.fit(X, y_train[0])\n",
    "\n",
    "        if mtype == \"adaboost\":\n",
    "            self.model = AdaBoostClassifier(DecisionTreeClassifier(max_depth=depth), n_estimators=n_estimators, random_state=0)\n",
    "            self.model.fit(X, y_train[0])\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        X = []\n",
    "        for i in range(len(self.models)):\n",
    "            prediction = self.models[i].predict(X_test[i])\n",
    "            X.append(pd.DataFrame(prediction))\n",
    "        X = pd.concat(X, axis=1)\n",
    "        return self.model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiperparámetros ya calculados:\n",
    "\n",
    "# Sin normalización\n",
    "# hiperparameters = {1: (2, 27), 2: (3, 24), 3: (2, 21), 4: (3, 30), \n",
    "#                     5: (2, 18), 6: (3, 30), 7: (2, 30), 8: (2, 27), \n",
    "#                     9: (2, 27), 10: (3, 15), 11: (2, 24), 12: (3, 15), \n",
    "#                     13: (2, 30), 14: (2, 27), 15: (2, 27)}\n",
    "\n",
    "# Con normalización\n",
    "hiperparameters = {1: (2, 27), 2: (3, 24), 3: (2, 21), 4: (3, 30), \n",
    "                   5: (2, 42), 6: (2, 36), 7: (2, 33), 8: (2, 42), \n",
    "                   9: (2, 45), 10: (3, 15), 11: (2, 24), 12: (3, 15), \n",
    "                   13: (2, 42), 14: (2, 27), 15: (2, 45)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6693208430913349\n",
      "[[7098 3577]\n",
      " [3483 7192]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.66      0.67     10675\n",
      "           1       0.67      0.67      0.67     10675\n",
      "\n",
      "    accuracy                           0.67     21350\n",
      "   macro avg       0.67      0.67      0.67     21350\n",
      "weighted avg       0.67      0.67      0.67     21350\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Ahora, hacemos el meta-ensamble:\n",
    "modelLogistic = MetaEnsamble(hiperparameters)\n",
    "modelLogistic.fitAdaBoost(X_train, y_train)\n",
    "modelLogistic.fit(X_train, y_train, mtype=\"logistic\")\n",
    "\n",
    "y_pred = modelLogistic.predict(X_val)\n",
    "print(accuracy_score(y_val[0], y_pred))\n",
    "print(confusion_matrix(y_val[0], y_pred))\n",
    "print(classification_report(y_val[0], y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6645433255269321\n",
      "[[7057 3618]\n",
      " [3544 7131]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.66      0.66     10675\n",
      "           1       0.66      0.67      0.67     10675\n",
      "\n",
      "    accuracy                           0.66     21350\n",
      "   macro avg       0.66      0.66      0.66     21350\n",
      "weighted avg       0.66      0.66      0.66     21350\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Ahora, hacemos el meta-ensamble:\n",
    "modelMLP = MetaEnsamble(hiperparameters)\n",
    "modelMLP.fitAdaBoost(X_train, y_train)\n",
    "modelMLP.fit(X_train, y_train, mtype=\"mlp\")\n",
    "\n",
    "y_pred = modelMLP.predict(X_val)\n",
    "print(accuracy_score(y_val[0], y_pred))\n",
    "print(confusion_matrix(y_val[0], y_pred))\n",
    "print(classification_report(y_val[0], y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6462657441901721\n",
      "[[7341 3933]\n",
      " [4043 7231]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.65      0.65     11274\n",
      "           1       0.65      0.64      0.64     11274\n",
      "\n",
      "    accuracy                           0.65     22548\n",
      "   macro avg       0.65      0.65      0.65     22548\n",
      "weighted avg       0.65      0.65      0.65     22548\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test Logistic\n",
    "\n",
    "y_pred = modelLogistic.predict(X_test)\n",
    "print(accuracy_score(y_test[0], y_pred))\n",
    "print(confusion_matrix(y_test[0], y_pred))\n",
    "print(classification_report(y_test[0], y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.640722015256342\n",
      "[[7283 3991]\n",
      " [4110 7164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.65      0.64     11274\n",
      "           1       0.64      0.64      0.64     11274\n",
      "\n",
      "    accuracy                           0.64     22548\n",
      "   macro avg       0.64      0.64      0.64     22548\n",
      "weighted avg       0.64      0.64      0.64     22548\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test MLP\n",
    "\n",
    "y_pred = modelMLP.predict(X_test)\n",
    "print(accuracy_score(y_test[0], y_pred))\n",
    "print(confusion_matrix(y_test[0], y_pred))\n",
    "print(classification_report(y_test[0], y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
